import geom
import geogrid
a = geogrid.geom.LngLat()
a = geogrid.geom.LngLat(0,0)
a.lng
a.lng()
geogrid.geom.quad(0,0)
geogrid.GeoGrid.quad(0,0)
import numpy
a = [[1,2,3],[4,5,6]]
a
a = numpy.array(a)
a
a.flatten()
vec = numpy.array([1,2,3])
numpy.array([2]) * vec
numpy.array([2, 3, 4]) * vec
[1,2].extend([3,4])
a = [1,2].extend([3,4])
a = [1,2]
a
a.extend(None)
ls
exit()
import src.python.lib.plotter
[1,2] is list
a = [1,2]
a is list
type(a)
type(a) is list
ls
exit()
import cat_gen
cat_gen.get_cats()
d = {'abc': 5}
d.get('abc')
d.get('abcd')
d.get('abcd') in [1]
exit()
import cat_gen
cat_gen.get_cats()
exit()
cat_gen.get_cats()
import cat_gen
cat_gen.get_cats()
cats = cat_gen.get_cats()
'location_services' in cats
"and" is not "and"
"and" is not "andd"
exit()
import geom
exit()
import geom
import geogrid
geogrid.geom.LngLat(1,1)
exit()
import random
random.normalvariate()
random.normalvariate(0,1)
import numpy
numpy.random.normal()
exit()
import testing
exit()
import testing
from testing import T
import yelp.testing
exit()
import testing
exit()
from yelp_lib.testing import T
import yelp.testing
from yelp.testing import T
import testify as T
exit()
exit)_
exit()
import EPI
import EPI.src.python.models.optimal_gaussian_process_linked_cpp
exit()
import optimal_learning
exit()
import optimal_learning.EPI.src.python.models.optimal_gaussian_process_linked_cpp
exit()
import numpy
numpy.float64(1.)
exit()
import numpy
numpy.random.uniform(2)
numpy.random.uniform(size=2)
numpy.random.uniform(size=2)*numpy.random.uniform(size=2)
numpy.diag(3)
numpy.diag([1,2])
a = numpy.array([1,2,3])
a
b = numpy.array([1,2,3])
a == b
a === b
b = numpy.array([1,2,3.0])
a == b
sum(a == b) == len(a)
(a==b).any()
(a==b).all()
(a!=b).all()
a = numpy.array([1,2,3])
b = {a:1}
a = 1
str(a)
str(a, 8)
"%8f" % a
from scipy.optimize.optimize import fminbound
exit()
l
p self.time_period
len(                                                                                                                                                                                                                                                          
len(self.time_period)
p self.time_period.total_days
p self.time_period.total_days()
p self.first_bid_time_period.total_days()
c
l
one_criteria[0]
one_criteria.category_aliases
q
l
q
l
one_cached_ad_asset.
one_cached_ad_asset.location.
one_cached_ad_asset.location.display
one_cached_ad_asset.location.display()
one_cached_ad_asset.location.city
one_cached_ad_asset.location.state
one_cached_ad_asset.location.country
one_cached_ad_asset.location.
one_cached_ad_asset.creative.type
one_cached_ad_asset.creative.type.__str__()
q
p search_point
q
import clog
import clog_util
exit
exit()
import clog
import clog_util
import clog
exit
exit()
import clog
exit()
import datetime
t = datetime.datetime.today()
t
t.hour()
t.hour
t.day
t.weekday
t.weekday()
"{0:.2f}".format(13.949999999999999)
import simplejson
us = set()
us.add(1)
us.add(2)
us.add(1)
us
us.add([1,3])
us.union(set([1,3]))
us
us.update(set([1,3]))
us
a = {"a": 2, "b":3}
len(a)
exit()
import optimal_learning
exit()
import clog
clog.log_line
exit()
import yelp_servlib.clog_util as clog
clog.log_line
exit()
import numpy
numpy.random.uniform()
numpy.random.uniform(size=10)
numpy.random.uniform(-1,1)
doc(numpy.random.uniform)
numpy.random.uniform.__doc__
print numpy.random.uniform.__doc__
numpy.random.uniform(-1,1,10)
import testify as T
print T.assert_almost_equal.__doc__
numpy.double.min
numpy.double.min()
numpy.finfo(numpy.float64)
numpy.finfo(numpy.float64).tiny
numpy.linalg.cond(numpy.array([[1,2],[1,4]])
)
numpy.linalg.cond(numpy.array([[1,0],[0,1e-10]]))
numpy.linalg.cond(numpy.array([[1,0],[0,1e-11]]))
numpy.linalg.cond(numpy.array([[1,0],[0,1e-308]]))
numpy.linalg.cond(numpy.array([[1,0],[0,1e-309]]))
numpy.linalg.cond(numpy.array([[1,0],[0,1e-310]]))
numpy.linalg.cond(numpy.array([[1,0],[0,1e-309]])) == numpy.linalg.cond(numpy.array([[1,0],[0,1e-310]]))
import random
print random.sample.__doc__
random.sample([1,2,3,4,5], 2)
1e-309 == 1e-311
1/1e-309 == 1/1e-311
1e-309*1e-309
1e-309*1e309
1e309/1e309
1e-309/1e-309
1e-400/1e-400
1e-310/1e-310
1e-311/1e-311
1e-312/1e-312
1e-324/1e-324
1e-323/1e-323
numpy.linalg.det(numpy.array([[1,2],[3,4]]))
numpy.linalg.cholesky
import scipy
import scipy.linalg
a = set([1])
a
list(a)
'a' in None
1 in a
import geogrid
for i in range(100): print 9*10**i
clear
cls
cls()
clear()
import os
os.path
os.path.abspath('.')
import numpy
xrange(1, 10, 0.5)
xrange(1, 10, 2)
xrange(1, 11, 2)
a = 1
a
numpy.arange(0, 1, 0.1)
import tests.EPI.src.python.models.plottable_optimal_gaussian_process_test
exit()
import numpy
numpy.__version__
exit()
import geogrid
exit()
self.category_yelp.category_maps.root_map_by_alias
c
exit()
q
exit()
import geom
import Geogrid.geom
exit()
line = "[4, 0, [-108.55623723109716, 45.785576854164255], []]   [[null, null, null, 0]]"
line.split("\t")
line.split("]]"
)
line.split("]]")
line.split("]]").split("[")
line.split("]]")[0].split("[")
line.split("]]")[0].split("[")[3]
line.split("]]")[0].split("[")[2]
line.split("]]")[0].split("[")[2][:-2]
line.split("]]")[0].split("[")[2][:-3]
line.split("]]")[0].split("[")[2][:-3].split(", ")
%run biz_details_query_viewer_script.py
line
line.split('[[')
line.split('[[')[1].split(']')
line.split('[[')[1].split(']')[0][:-1]
line.split('[[')[1].split(']')[0][-1]
line.split('[[')[1].split(']')[:-2][-1]
line.split('[[')[1].split(']')[:-2]
%run biz_details_query_viewer_script.py
line.split('[[')[1].split(']')[:-2]
%run biz_details_query_viewer_script.py
line.split('[[')[1].split(']')[:-2]
list(line.split('[[')[1].split(']')[:-2])
list(line.split('[[')[1].split(']')[:-2])[0]
%run biz_details_query_viewer_script.py
line = [4, 12, [-118.24641881958932, 34.041479913335429], ["seafoodmarkets"]]  [[2990948, "seafoodmarket│    """                                                                                                                             
s", [-118.4836988, 34.235606900000001], 0], [2990948, "seafoodmarkets", [-118.4836988, 34.235606900000001], 0]]                          │    # Make the figure
line = "[4, 12, [-118.24641881958932, 34.041479913335429], ["seafoodmarkets"]]  [[2990948, "seafoodmarkets", [-118.4836988, 34.235606900000001], 0], [2990948, "seafoodmarkets", [-118.4836988, 34.235606900000001], 0]]"
line = '[4, 12, [-118.24641881958932, 34.041479913335429], ["seafoodmarkets"]]  [[2990948, "seafoodmarkets", [-118.4836988, 34.235606900000001], 0], [2990948, "seafoodmarkets", [-118.4836988, 34.235606900000001], 0]]'
line
list(line.split('[[')[1].split(']')[:-2])[0]
line.split('[[')[1].split(']')[:-2]
line.split('[[')[1]
line.split('[[')[1].split("], [")
line.split('[[')[1].split("]]")[0].split("], [")
%run biz_details_query_viewer_script.py
list('[' + 'null, null, null, 0' + ']')
ad_info
ai = 'null, null, null, 0'
list(ai)
ai = '[' + ai + ']'
ai
list(ai)
import ast
ast.literal_eval(ai)
ai
ai = 'null, null, null, 0'
ai[0:4]
ai[0:4] == 'null'
%run biz_details_query_viewer_script.py
line = '[4, 12, [-118.24641881958932, 34.041479913335429], ["seafoodmarkets"]]  [[2990948, "seafoodmarkets", [-118.4836988, 34.235606900000001], 0], [2990948, "seafoodmarkets", [-118.4836988, 34.235606900000001], 0]]'
line.split("]]")
line.split("]]")[0].split("[")[-1]
%run biz_details_query_viewer_script.py
line.split("]]")[0].split("[")[-1]
line.split("]]")[0].split("[")[-1].split(", ")
t = '"mediterranean", "pakistani", "indpak"'
t.split(", ")
t = "mediterranean"
t = '"mediterranean"'
t
str(t)
t[1:-1]
%run biz_details_query_viewer_script.py
ls
exit()
%run dataset_challenge_email_to_csv_script.py
line = "Name: Dong Su"
line.split("Name: "
)
%run dataset_challenge_email_to_csv_script.py
ls
%run dataset_challenge_email_to_csv_script.py
%run /nail/home/sclark/scratch/biz_details_query_viewer_script.py
line = '[4, 0, [-0.17802360735018138, 38.968735700621316], ["framing"]] [[null, null, null, 0]]'
line.split('[')[1]
line.split('[')[1].split(', ')
import /nail/home/sclark/scratch/biz_details_query_viewer_script.py
exit()
import impression_recovery
ad_info
ad_info = get_ad_info(line)
line = '[4, 0, [-0.17802360735018138, 38.968735700621316], ["framing"]] [[null, null, null, 0]]'
ad_info = get_ad_info(line)
ad_info = impression_recovery.get_ad_info(line)
ad_info
ad_info[0].split(', ')
ad_info = '2292387, "preschools", [-111.790638, 33.389921700000002], 0'
ad_info.split(", [")
ad_info.split(", [")[1]
ad_info.split(", [")[1].split("], ")
ad_info.split(", [")[1].split("], ")[0]
ad_info.split(", [")[1].split("], ")[0].split(", ")
%run impression_recovery.py
import yelp.ad.config_ad
yelp.ad.config_ad.default_category_radius
import geogrid
geogrid.geom.item_inside_lng_lat_radius.__doc__
print geogrid.geom.item_inside_lng_lat_radius.__doc__
%run impression_recovery.py
import scipy.spatial.KDTree
import scipy.spatial
scipy.spatial.KDTree
print scipy.spatial.KDTree.__doc__
fake = scipy.spatial.KDTree(numpy.array([[1,1], [1,0], [0,1], [0,0]]))
fake.query_ball_point(numpy.array([0,0], 1)
)
fake.query_ball_point(numpy.array([0,0]), 1)
fake.query_ball_point(numpy.array([0,0]), 2)
fake.query_ball_point(numpy.array([0,0]), 0.5)
fake.query_ball_point(numpy.array([0,0,0]), 0.5)
%run impression_recovery.py
exit()
%run impression_recovery.py
"%.4f" % 0.1234567
"%.2f" % 123.1234567
"%.2f" % -123.1234567
round(-123.1234567, 2)
line = '["[-0.68, 3545.32]", ["petstore"]]      [[2906409, "petstore", "[-8.75, 3532.95]", 0]]'
line.split("]]")[0].split("[")[2][:-3].split(", ")
line.split("]]")[0].split("[")[2][:-4].split(", ")
line.split('[[')[1].split("]]")[0].split("], [")
line = '["[-0.84, 3538.80]", ["deptstores", "shoppingcenters"]] [[2143066, "shopping", "[-2.73, 3544.21]", 0], [2551476, "shopping", "[-8.58, 3539.52]", 0]]'
line.split('[[')[1].split("]]")[0].split("], [")
ad_info = line.split('[[')[1].split("]]")[0].split("], [")
ad_info.split(", [")[1].split("], ")[0].split(", ")
ad_info.split(", [")[1].split("], ")[0]
ad_info.split(", [")[1]
ad_info[0].split(", [")[1].split("], ")[0].split(", ")
ad_info[0].split(", [")
ad_info[0].split(", "[")
ad_info[0].split(', "[')
ad_info[0].split(', "[')[1].split(']"')[0]
ad_info[0].split(', "[')[1].split(']"')[0].split(", ")
%run impression_recovery.py
line = '["[-0.84, 3538.80]", ["deptstores", "shoppingcenters"]] [0, [[2143066, "shopping", "[-2.73, 3544.21]", 0], [2551476, "shopping", "[-8.58, 3539.52]", 0]]]'
line.split('[[')[1].split("]]")[0].split("], [")
line.split('[[')[0].split("]]")[1]
line.split('[[')[0].split("]]")[1].split("[")[1]
line.split('[[')[0].split("]]")[1].split("[")[1][:-2]
%run impression_recovery.py
line.split('[[')[1].split("]]")[0].split("], [")
line = [1, []]
line.split('[[')[1].split("]]")[0].split("], [")
line.split(']]')
line = '["[-1.47, 3539.25]", ["dentists"]]      [1, []]'
line.split('[[')[1].split("]]")[0].split("], [")
line.split(']]')
line
line = '["[-0.84, 3538.80]", ["deptstores", "shoppingcenters"]] [0, [[2143066, "shopping", "[-2.73, 3544.21]", 0], [2551476, "shopping", "[-8.58, 3539.52]", 0]]]'
line.split(']]')
line.split('\t')
line.split('[[
line.split('[[')
line = '["[-1.47, 3539.25]", ["dentists"]]      [1, []]'
line.split('[[')
len(line.split('[['))
line.split('[[')[1].split("]]")[0].split("], [")
line
line.split(' []]')[0]
line.split(', []]')[0].split('[')[1]
line.split(', []]')[0].split('[')
line.split(', []]')[0].split('[')[-1]
%run impression_recovery.py
line = '["[-0.17, 3162.67]", []]        [1, []]'
line.split(', []]')[0].split('[')[-1]
line.split(', []]')[-1].split('[')[-1]
line.split(', []]')
line.split(', []]')[-1]
line.split(', []]')[-2]
line.split(', []]')[-2].split('[')[-1]
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/nail/home/sclark/mr_out/biz_details_query_stream/biz_details_query_stream_031513to031613")
file_name
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/nail/home/sclark/mr_out/biz_details_query_stream/biz_details_query_stream_031513to031613")
exit()
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/nail/home/sclark/mr_out/biz_details_query_stream/biz_details_query_stream_031513to031613")
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("opp_stream_new.txt")
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("opp_stream_new.txt"
)
line = '["[-10.9, 3537.3]", [822]]      [0, [[2951447, [822], "[-9.0, 3536.8]", 0], [2951447, [822], "[-9.0, 3536.8]", 0]]]'
line.split("]]")[0].split("[")[-1].split(", ")
line = '["[-10088.0, 1354.4]", [810, 800, 1013, 817]]   [3, []]'
line.split("]]")[0].split("[")[-1].split(", ")
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("opp_stream_new.txt")
bad_impressions_by_category.keys()
bad_impressions_by_category['151']
bad_impressions_by_category['150']
bad_impressions_by_category['822']
len(bad_impressions_by_category['822'])
len(bad_impressions_by_category['822']['ids)
len(bad_impressions_by_category['822']['ids'])
%run impression_recovery.py
len(bad_impressions_by_category['822']['ids'])
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/scratch/sclark/opportunity_stream/all_opps_stream_03152013")
len(bad_impressions_by_category['822']['ids'])
len(bad_impressions_by_category[822]['ids'])
len(bad_impressions_by_category.keys())
bad_impressions_by_category.keys()
len(bad_impressions_by_category[0]['ids'])
len(bad_impressions_by_category[1]['ids'])
len(bad_impressions_by_category[2]['ids'])
len(bad_impressions_by_category[3]['ids'])
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/scratch/sclark/opportunity_stream/all_opps_stream_03152013.head")
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/scratch/sclark/opportunity_stream/all_opps_stream_03152013.head")
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/scratch/sclark/opportunity_stream/all_opps_stream_03152013.head")
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/scratch/sclark/opportunity_stream/all_opps_stream_03152013.head")
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/scratch/sclark/opportunity_stream/all_opps_stream_03152013.head")
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/scratch/sclark/opportunity_stream/all_opps_stream_03152013.head")
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/scratch/sclark/opportunity_stream/all_opps_stream_03152013.head")
%run impression_recovery.py
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/scratch/sclark/opportunity_stream/all_opps_stream_03152013.head")
file_name, bad_impressions_by_category, bad_impressions_by_id = read_file("/scratch/sclark/opportunity_stream/all_opps_stream_03152013")
len(bad_impressions_by_category[822]['ids'])
%run impression_recovery.py
recover_bad_impressions_old(missed_opportunites, bad_impressions_by_category, bad_impressions_by_id)
build_KDtrees(bad_impressions_by_category)
%run impression_recovery.py
build_KDtrees(bad_impressions_by_category)
bad_impressions_by_category
bad_impressions_by_category.keys()
bad_impressions_by_category[1].keys()
bad_impressions_by_category[1590].keys()
bad_impressions_by_category[1590]['ids']
bad_impressions_by_category[1590]['locs']
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
bad_impressions_by_category[1590]['locs'][:]
location_array = numpy.array(bad_impressions_by_category[1590]['locs'][:])
location_array
location_array - numpy.array([1000, 1000])
(location_array - numpy.array([1000, 1000]))**2
(location_array - numpy.array([1000, 1000]))**2 < 100**2
sum((location_array - numpy.array([1000, 1000]))**2) < 100**2
sum((location_array - numpy.array([1000, 1000]))**2)
sum((location_array - numpy.array([1000, 1000]))**2.T)
sum(((location_array - numpy.array([1000, 1000]))**2).T)
sum(((location_array - numpy.array([1000, 1000]))**2).T) < 100**2
numpy.nonzero(sum(((location_array - numpy.array([1000, 1000]))**2).T) < 100**2)
numpy.nonzero(sum(((location_array - numpy.array([1000, 1000]))**2).T) < 100000**2)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
build_KDtrees(bad_impressions_by_category)
import scipy.spatial
%run impression_recovery.py
build_KDtrees(bad_impressions_by_category)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
%run impression_recovery.py
recover_bad_impressions(file_name, bad_impressions_by_category, bad_impressions_by_id)
import scipy.spatial
exit()
import scipy.scipy
exit()
import scipy.spatial
exit()
import scipy.spatial
exit()
import Cython
Cython.__doc__
Cython.__path__
Cython.__name__
Cython.__package__
import pkg_resources
pkg_resources.get_distribution("Cython").version
exit()
import scipy
scipy.__version__
exit()
